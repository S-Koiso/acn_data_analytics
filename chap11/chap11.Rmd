---
title: "Chapter 11 Decision Tree"
author: "Satoshi Koiso"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an R Markdown document about Chapter 11 of [this book](https://www.shoeisha.co.jp/book/detail/9784798143446).





Clustering analysis is divided into two categories:

- Hierarchical Clustering
- Non-Hierarchical Clustering

## 10.1 Data Collection and Processing

```{r install dataset and scaling}
data.orig <- read.csv("http://enterprisezine.jp/static/images/article/6873/flat35_research_2013.csv", header=TRUE, row.names="都道府県", fileEncoding="UTF-8")

head(data.orig)

# scaling to make the average as 0 and std as 1
data.scale <- scale(data.orig)

```


## 10.2 Hierarchical Clustering

### The Ward Method
calculating the euclidean distances.
```{r ward}
data.dist <- dist(data.scale, method = "euclidean")
data.dist

# clustering
ward.hclust <- hclust(data.dist, method = "ward.D2")

# If you dont know which argument to choose,
#?hclust
```

### Visualization

```{r ward visual}
# visualize the dendrogram
plot(ward.hclust, hang = -1) # hang: to align the labels

# flip the dendrogram 90 degree
plot(as.dendrogram(ward.hclust), horiz = T, main = "Hierarchical Clustering Dendrogram (Ward)")

```

### Number of Clusters

To decide the number of clusters, capture the characteristics of each cluster by visualizing the data.
```{r heatmap}
# heatmap
heatmap(data.scale, Rowv = as.dendrogram(ward.hclust), Colv = NA, scale = "none", main = "Heat Map for Hierarcical Clustering (Ward)")

#category by # of cluster
ward.cutree <- cutree(ward.hclust, k=2:4)
ward.cutree
```

Compare the heatmap with the prefectures' characteristics by cluster No.
- 2 clusters: 
  - Cluster 1: 
    - Household size: L
    - Household income: H
    - House size: L
    - Asset amount: H
  - Cluster 2: 
    - Household size: H
    - Household income: L
    - House size: H
    - Asset amount: L
- 3 clusters: 
  - Cluster 1: 
    - Age: H
    - Household size: L
    - Household income: H
    - House size: L
    - Asset amount: H
  - Cluster 2: 
    - Age: H
    - Household size: H
    - Household income: L
    - House size: H
    - Asset amount: L
  - Cluster 3: 
    - Age: L
    - Household size: H
    - Household income: L
    - House size: H
    - Asset amount: L
- 4 clusters: Besides the 3 clusters, Tokyo is separated. 


### Cluster Boundaries
```{r plot with boundaries}
plot(ward.hclust, hang = -1)
rect.hclust(ward.hclust, k=3, border = "red")
```

## 10.3 Non-Hierarchical Clustering

K-Means clustering is one main example of non-hierarchical clustering.

### Number of Clusters

We need to choose the number of clusters **before** conducting the K-Means method.

```{r install package, include=FALSE}
install.packages("NbClust", repos = "http://cran.us.r-project.org")
library(NbClust)
```

```{r Evaluation the number of clusters}
km.best.nc <- NbClust(data.scale, distance = "euclidean", min.nc=2, max.nc = 10, method = "kmeans", index = "alllong")
```

Among the 30 indexes of clustering,
- Hubert: we need to check the number from the graph (below)
- Dindex: we need to check the number from the graph (below)
- PseudoT2: we cannot apply this index to non-hierarchical clustering.
- Frey: we cannot apply this index to non-hierarchical clustering.

Check the numbers of clusters suggested by Hubert and Dindex
```{r hubert and dindex}
# Hubert
NbClust(data.scale, distance = "euclidean", min.nc=2, max.nc = 10, method = "kmeans", index = "hubert") # 6

# Dindex
NbClust(data.scale, distance = "euclidean", min.nc=2, max.nc = 10, method = "kmeans", index = "dindex") #2
```

Check the numbers of clusters suggested by PeudoT2 and Frey.
```{r discard PeudoT2 and Frey}
km.best.nc$Best.nc
# 2 and 3
```

**Results**

| # of indexes | # of clusters |
|-------------:|--------------:|
|10 | 2 | 
| 6 | 3 | 
| 2 | 4 |
| 1 | 5 | 
| 1 | 6 |
| 4 | 7 | 
| 4 | 10 | 

2 is the best number of clusters.

### K-Means Clustering Analysis
```{r k-means}
km <- kmeans(data.scale, centers = 2, nstart = 10)
km
```


### Visualization
```{r visualization}
library(cluster)
clusplot(data.scale, km$cluster,color =T, lines=3,labels=2)

```
